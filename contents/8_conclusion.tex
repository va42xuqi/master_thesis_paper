\chapter{Conclusion}
\label{chapt:conclusion}
This Chapter revisits the research questions introduced at the beginning of this work. Each question is followed by an explanation based on the experimental results presented in Section \ref{sec:eval}.

\begin{itemize}
    \item[\textbf{Q1:}] \textbf{How do attention-based models compare to other models in general?} \\
    Experiment E1 \ref{exp:e1} revealed that attention-based models, particularly the Transformer, were highly competitive in almost every scenario. While the LMU consistently achieved the best overall accuracy across NBA and soccer datasets, the Transformer was often close behind, demonstrating that it is capable of capturing complex motion patterns effectively. Its performance indicates that, with further refinements, such as improved strategies for positional encoding, the Transformer has the potential to surpass recurrent architectures such as the LSTM and LMU in human motion forecasting.

    \item[\textbf{Q2:}] \textbf{How does model performance vary when positional or velocity data is excluded?} \\
    In Experiment E2 \ref{exp:e2}, excluding either positional or velocity data led to a noticeable decrease in performance for all models. However, the Transformer remained competitive and demonstrated resilience even when key data inputs were missing. The LSTM adapted well to the absence of positional information, achieving the highest accuracy in such cases, but the Transformer was not far behind. BitNet, on the other hand, performed poorly and would require significantly more parameters to match the competitive performance, making it impractical for this context. This highlights the need for optimization strategies when input data is reduced.

    \item[\textbf{Q3:}] \textbf{How robust is each model to missing historical information?} \\
    The results from Experiment E3 \ref{exp:e3} showed that the Transformer maintained consistent performance across varying lengths of historical information, showcasing its ability to extract relevant features even with limited context. While the LSTM benefited the most from longer historical data, the Transformer remained competitive across all settings, further indicating its adaptability in dynamic environments. Linear models excelled only with shorter histories and were outperformed by both the Transformer and LSTM when more extended historical contexts were available.

    \item[\textbf{Q4:}] \textbf{How does a multivariate predictor compare to multiple univariate predictors in terms of forecasting accuracy?} \\
    In Experiment E4 \ref{exp:e4}, multivariate predictors demonstrated superior performance compared to univariate predictors across all models. The Transformer, though showing less improvement in the univariate setting, remained highly effective with multivariate inputs, proving its ability to handle complex interactions between features. The LSTM, while benefiting slightly more from univariate predictions, did not show a consistent enough advantage to challenge the overall efficacy of the Transformer in multivariate scenarios.

    \item[\textbf{Q5:}] \textbf{How do the models generalize to other teams?} \\
    Experiment E5 \ref{exp:e5} demonstrated that the Transformer model was capable of generalizing well to unseen teams, performing comparably to the LMU and LSTM. Its adaptability to different team dynamics suggests that attention-based models are promising candidates for applications requiring forecasting across various contexts. This generalization ability is crucial for real-world implementations, where diverse and changing team structures are expected.

    \item[\textbf{Q6:}] \textbf{How does transfer learning impact model performance when trained on one domain and fine-tuned on another?} \\
    The results of Experiment E6 \ref{exp:e6} indicated that transfer learning had a positive impact on the Transformer’s performance, allowing it to adapt effectively to new domains when fine-tuned. This capability positions the Transformer as a versatile model for sports forecasting, capable of leveraging pre-existing knowledge from one sport to another, which makes it a valuable asset in scenarios with limited data availability.

    \item[\textbf{Q7:}] \textbf{How does each model perform when predicting the movement of a single target player using only that player’s data, excluding the social-interaction context?} \\
    Experiment E7 \ref{exp:e7} showed that all models, including the Transformer, experienced a decline in prediction accuracy when only the target player's data was used without considering social interactions. Nevertheless, the Transformer still performed competitively in this setting, suggesting that it retains a strong ability to forecast motion even in the absence of broader contextual information.
\end{itemize}
\newpage
\subsection*{Final Remarks}
The findings confirm that while the LMU and LSTM were top performers in some experiments, the Transformer remained a highly competitive alternative across almost all settings. This suggests that with further investigation, particularly in areas such as improved positional encoding strategies, the Transformer could potentially surpass the recurrent architectures. Meanwhile, BitNet's poor performance makes it impractical for this context unless significantly more parameters are introduced, which would counteract its efficiency. Overall, attention-based models demonstrate considerable potential for advancing human motion forecasting in dynamic environments.

\begin{comment}
\begin{itemize}
    \item[\textbf{Q1:}] \textbf{How do the models compare in terms of general accuracy across different datasets (NBA, DFL, and Soccer)?}\ Results from Experiment \ref{exp:init} show that the Transformer model demonstrated the most consistent performance across all datasets, particularly excelling in the \gls{nba} and Soccer domains. The \gls{lmu} model performed exceptionally well on the \gls{dfl} dataset, highlighting its strength in handling domain-specific dynamics. The Transformer outperformed all other models in the \gls{nba} dataset, while in Soccer, it was a close competitor to the \gls{lmu}, depending on the data configurations used. BitNet consistently underperformed relative to the Transformer and \gls{lmu} across all datasets, with noticeable degradation in complex scenarios, particularly those in the Soccer dataset. Linear models performed poorly overall, only outperforming the \gls{lstm} on the Soccer dataset.
    \item[\textbf{Q2:}] \textbf{How does the model's performance vary when using only positional data, only velocity data, or both positional and velocity data?} \\
    Results from Experiment \ref{exp:pos_vel} indicate that the \gls{lmu} and Transformer models were the most resilient to missing positional or velocity data. The Transformer outperformed on the \gls{nba} dataset, while the \gls{lmu} performed better on the \gls{dfl} dataset. Overall, using velocity-only data yielded better results than position-only data. In Soccer, the velocity-only input even surpassed the full input version, whereas in \gls{nba}, the full input performed best. The BitNet model was the most affected by missing input types.

    \item[\textbf{Q3:}] \textbf{What is the effect of varying the length of historical context and forecast horizon on the model's accuracy?} \\
    Experiment \ref{exp:history_forcast} demonstrated that while BitNet performed well and remained stable with shorter input lengths for \gls{nba}, it struggled with larger context windows. The Transformer achieved the best overall performance, except on the \gls{dfl} dataset with a 2-second history, where the \gls{lmu} outperformed it. Recurrent models were significantly affected by limited context information.

    \item[\textbf{Q4:}] \textbf{How does a multivariate predictor compare to multiple univariate predictors in terms of forecasting accuracy?} \\
    Results from Experiment \ref{exp:uni_multi} suggest that in \gls{nba}, all models struggled with univariate models for each dimension. In the Soccer dataset, only the Transformer had difficulty handling separate variables, while other models showed slight improvements across all metrics. Notably, the \gls{lstm} benefited significantly from the separation of information into multiple models.

    \item[\textbf{Q5:}] \textbf{What are the outcomes when training the model on Team A and testing the model on Team A versus training it on Team A and testing it on Team B?}\\
    Results from Experiment \ref{exp:intra_inter} indicate that all models generalize well to other teams, with only a slight increase in all metrics. Specifically, the \gls{lmu} demonstrated consistent performance on the \gls{dfl} dataset, achieving the same \gls{ade} for both teams and a minor increase of 0.02 meters in \gls{fde} in the inter-team scenario. The intra-team scenario refers to training and testing on the same team, while the inter-team scenario involves training on one team and testing on a different team.

    \item[\textbf{Q6:}] \textbf{How does transfer learning impact model performance when trained on one domain and fine-tuned on another?}\\
    To evaluate transfer learning, both models were modified to use only one player as input, creating a useful domain for comparison. Results from Experiment \ref{exp:transf} show that the data set \gls{nba} did not benefit from pretraining on the other domain, and in some cases, pretrained models performed 0slightly worse than those trained only on \gls{nba} data, except for the Transformer model. The Transformer improved with fine-tuning and outperformed the \gls{lmu}, making it the best predictor for \gls{nba} with single-player input. For the Soccer dataset, transfer learning slightly decreased performance in all recurrent models, while linear models, the Transformer, and BitNet saw slight improvements. Notably, BitNet was outperformed by linear models in both cases. Overall, the normally trained \gls{lmu} performed best for \gls{nba}, and the pretrained one-layer linear model performed unexpectedly well on the \gls{dfl} dataset.

    \item[\textbf{Q7:}] \textbf{How does the model's performance differ when predicting the movement of a single target player using only that player’s data compared to using data from all players?}\\
    Experiment \ref{exp:single_vs_all} reveals that in the \gls{nba} domain, linear models performed better when predicting with only the single player’s data. In contrast, other models benefited slightly from including data from all players due to player interactions and additional context. For the \gls{dfl} dataset, attention-based models like the Transformer and BitNet showed improved performance with additional context, while other models performed better with single-player data. Despite this, BitNet was outperformed by \gls{lstm}, \gls{lmu}, and Transformer, making it less favorable. The \gls{lstm} benefited from reduced distraction and, although it did not surpass \gls{lmu}, remained a strong competitor. The Transformer model, when using only one player as input, competed primarily with linear models, still outperforming BitNet.
\end{itemize}

\textbf{Final Remarks:}
Overall, the Transformer and \gls{lmu} models demonstrated the strongest performance across various scenarios. The Transformer excelled with complex data and varied contexts, while the \gls{lmu} showed robust results, especially in the \gls{dfl} dataset. The \gls{lstm} performed well with single-player data and benefited from multi-model approaches, though it did not surpass the Transformer or \gls{lmu}. The BitNet model generally underperformed compared to these top models, and linear models were effective for simpler tasks but lacked the adaptability of more complex models.
\end{comment}