\chapter{Conclusion}
\label{sec:conclusion}
This section revisits the research questions introduced at the beginning of this work. Each question is followed by an explanation based on the experimental results presented in Section \ref{sec:eval}.

\begin{itemize}
    \item[\textbf{Q1:}] \textbf{How does the model's performance vary when using only positional data, only velocity data, or both positional and velocity data?} \\
    Results from Experiment \ref{exp:pos_vel} indicate that the \gls{lmu} and Transformer models were the most resilient to missing positional or velocity data. The Transformer outperformed on the \gls{nba} dataset, while the \gls{lmu} performed better on the \gls{dfl} dataset. Overall, using velocity-only data yielded better results than position-only data. In Soccer, the velocity-only input even surpassed the full input version, whereas in \gls{nba}, the full input performed best. The BitNet model was the most affected by missing input types.

    \item[\textbf{Q2:}] \textbf{What is the effect of varying the length of historical context and forecast horizon on the model's accuracy?} \\
    Experiment \ref{exp:history_forcast} demonstrated that while BitNet performed well and remained stable with shorter input lengths for \gls{nba}, it struggled with larger context windows. The Transformer achieved the best overall performance, except on the \gls{dfl} dataset with a 2-second history, where the \gls{lmu} outperformed it. Recurrent models were significantly affected by limited context information.

    \item[\textbf{Q3:}] \textbf{How does a multivariate predictor compare to multiple univariate predictors in terms of forecasting accuracy?} \\
    Results from Experiment \ref{exp:uni_multi} suggest that in \gls{nba}, all models struggled with univariate models for each dimension. In the Soccer dataset, only the Transformer had difficulty handling separate variables, while other models showed slight improvements across all metrics. Notably, the \gls{lstm} benefited significantly from the separation of information into multiple models.

    \item[\textbf{Q4:}] \textbf{What are the outcomes when training the model on Team A and testing the model on Team A versus training it on Team A and testing it on Team B?}\\
    Results from Experiment \ref{exp:intra_inter} indicate that all models generalize well to other teams, with only a slight increase in all metrics. Specifically, the \gls{lmu} demonstrated consistent performance on the \gls{dfl} dataset, achieving the same \gls{ade} for both teams and a minor increase of 0.02 meters in \gls{fde} in the inter-team scenario. The intra-team scenario refers to training and testing on the same team, while the inter-team scenario involves training on one team and testing on a different team.

    \item[\textbf{Q5:}] \textbf{How does transfer learning impact model performance when trained on one domain and fine-tuned on another?}\\
    To evaluate transfer learning, both models were modified to use only one player as input, creating a useful domain for comparison. Results from Experiment \ref{exp:transf} show that the \gls{nba} dataset did not benefit from pretraining on the other domain, and in some cases, pretrained models performed slightly worse than those trained only on \gls{nba} data, except for the Transformer model. The Transformer improved with fine-tuning and outperformed the \gls{lmu}, making it the best predictor for \gls{nba} with single-player input. For the Soccer dataset, transfer learning slightly decreased performance in all recurrent models, while linear models, the Transformer, and BitNet saw slight improvements. Notably, BitNet was outperformed by linear models in both cases. Overall, the normally trained \gls{lmu} performed best for \gls{nba}, and the pretrained one-layer linear model performed unexpectedly well on the \gls{dfl} dataset.

    \item[\textbf{Q6:}] \textbf{How does the model's performance differ when predicting the movement of a single target player using only that player’s data compared to using data from all players?}\\
    Experiment \ref{exp:single_vs_all} reveals that in the \gls{nba} domain, linear models performed better when predicting with only the single player’s data. In contrast, other models benefited slightly from including data from all players due to player interactions and additional context. For the \gls{dfl} dataset, attention-based models like the Transformer and BitNet showed improved performance with additional context, while other models performed better with single-player data. Despite this, BitNet was outperformed by \gls{lstm}, \gls{lmu}, and Transformer, making it less favorable. The \gls{lstm} benefited from reduced distraction and, although it did not surpass \gls{lmu}, remained a strong competitor. The Transformer model, when using only one player as input, competed primarily with linear models, still outperforming BitNet.
\end{itemize}

\textbf{Final Remarks:}
Overall, the Transformer and \gls{lmu} models demonstrated the strongest performance across various scenarios. The Transformer excelled with complex data and varied contexts, while the \gls{lmu} showed robust results, especially in the \gls{dfl} dataset. The \gls{lstm} performed well with single-player data and benefited from multi-model approaches, though it did not surpass the Transformer or \gls{lmu}. The BitNet model generally underperformed compared to these top models, and linear models were effective for simpler tasks but lacked the adaptability of more complex models.
