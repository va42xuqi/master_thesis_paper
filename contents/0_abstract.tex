\begin{abstract}{german}
Die Vorhersage menschlicher Bewegungen spielt eine entscheidende Rolle in Bereichen wie der Sportanalyse, der Verletzungsprävention und der Optimierung von Spielstrategien. Die genaue Vorhersage von Bewegungen in dynamischen, Echtzeitumgebungen, wie beispielsweise bei Basketballspielen, stellt jedoch eine erhebliche Herausforderung dar, da menschliche Bewegungen oft unvorhersehbar und stark variabel sind. Traditionelle Vorhersagemodelle, wie Kalman- und Partikelfilter, erweisen sich in solchen Kontexten häufig als ineffektiv, da sie die nichtlinearen und stochastischen Eigenschaften schneller Sportarten nicht erfassen können. Neuere Fortschritte in Deep-Learning-Modellen, insbesondere rekurrente neuronale Netze (RNNs) wie Long Short-Term Memory (LSTM) und Legendre Memory Units (LMUs), haben vielversprechende Ansätze gezeigt, diese Einschränkungen zu überwinden. Dennoch haben auch diese Modelle Schwierigkeiten mit der Erinnerung an lange Zeitfolgen und der Verarbeitung großer Eingabedatensätze.

Diese Arbeit untersucht das Potenzial von Transformer-basierten Modellen, die durch ihren Self-Attention-Mechanismus bekannt sind, um die Genauigkeit und Robustheit der Vorhersage menschlicher Bewegungen zu verbessern. Der Self-Attention-Mechanismus ermöglicht es Transformern, langreichweitige Abhängigkeiten und Nichtlinearitäten in den Daten zu erfassen, was sie besonders geeignet für komplexe, multivariate und Multi-Task-Bewegungsvorhersagen macht. Wir vergleichen die Leistung von LSTM-, LMU- und Transformer-Architekturen anhand von dynamischen Sportdatensätzen, mit besonderem Fokus auf die Bewegungen von NBA-Spielern. Insbesondere untersuchen wir, wie diese Modelle mit unterschiedlichen historischen Kontexten, Vorhersagehorizonten und Eingabedatenkonfigurationen – einschließlich positionsbasierter, geschwindigkeitsbasierter und kombinierter Eingaben – umgehen. Darüber hinaus analysiert die Arbeit den Einfluss multivariater gegenüber univariater Modellierungsansätze sowie die Generalisierbarkeit dieser Modelle über verschiedene Teams hinweg unter Verwendung von Transfer-Learning-Techniken.

Unsere Ergebnisse zeigen, dass Transformer-Modelle die traditionellen rekurrenten Architekturen hinsichtlich Genauigkeit, Robustheit und Vorhersagehorizont bei der Anwendung auf multivariate Sportdaten übertreffen. Allerdings gehen sie auch mit einer höheren Rechenkomplexität einher. Es zeigte sich zudem, dass die Kombination von Positions- und Geschwindigkeitsdaten zu genaueren Vorhersagen führt und multivariate Modelle gegenüber univariaten Ansätzen überlegen sind. Durch eine Reihe von Ablationsstudien identifizieren wir die optimalen Konfigurationen für die Vorhersage menschlicher Bewegungen in Sportanwendungen. Diese Arbeit liefert Einblicke, wie KI-Modelle, insbesondere Transformer, an Echtzeit-Umgebungen mit hohen Anforderungen angepasst werden können, und bildet die Grundlage für zukünftige Entwicklungen im Bereich der Sportanalytik.
%Die Vorhersage von menschlichen Bewegungen in dynamischen Umgebungen, wie zum Beispiel im Sport, stellt aufgrund der unvorhersehbaren und stark variablen Natur der Bewegungen eine große Herausforderung dar. Eine genaue Vorhersage von Bewegungen ist entscheidend, um Spielstrategien zu verbessern, Verletzungen zu vermeiden und Echtzeitanalysen zu ermöglichen. Traditionelle Methoden wie der Kalman-Filter und der Partikel-Filter sind oft nicht in der Lage, die nichtlinearen Muster in schnelllebigen Sportumgebungen zu erfassen. Um diese Einschränkungen zu überwinden, untersucht diese Arbeit den Einsatz fortschrittlicher maschineller Lernmodelle, insbesondere auf Transformern basierende Architekturen, zur Vorhersage menschlicher Bewegungen.

%Transformer, die auf Self-Attention-Mechanismen basieren, haben sich im Vergleich zu traditionellen rekurrenten neuronalen Netzen wie dem Long Short-Term Memory und den Legendre Memory Units als überlegen bei der Verarbeitung langer Sequenzen und komplexer Datenabhängigkeiten erwiesen. Diese Arbeit bewertet und vergleicht die Leistung von Transformermodellen, einschließlich des neu eingeführten BitNet, mit Long Short-Term Memory und Legendre Memory Units unter Verwendung von Daten aus dynamischen Sportumgebungen, insbesondere aus Basketballspielen der nordamerikanischen Basketball-Profiliga. Wichtige Metriken wie der durchschnittliche absolute Fehler, der mittlere quadratische Fehler und der durchschnittliche Versetzungsfehler werden zur Beurteilung der Genauigkeit und Robustheit herangezogen.

%Die Forschung untersucht außerdem den Einfluss verschiedener Datentypen (Position im Vergleich zu Geschwindigkeit), der Länge des historischen Kontextes und des Vorhersagehorizonts auf die Leistung der Modelle. Die Ergebnisse zeigen, dass auf Transformern basierende Modelle traditionelle rekurrente neuronale Netze übertreffen, insbesondere in Szenarien mit hoher Datenkomplexität und Interaktionen zwischen mehreren Spielern. Darüber hinaus werden die Vorteile multivariater Modelle gegenüber univariaten Modellen sowie die rechnerische Effizienz der verschiedenen Architekturen untersucht.

%Diese Arbeit trägt zur Sportanalyse bei, indem sie das Potenzial von Transformermodellen zur Vorhersage menschlicher Bewegungen aufzeigt und damit die Grundlage für zukünftige Entwicklungen in der Echtzeitbewegungsvorhersage und taktischen Entscheidungsfindung im Sport legt.
\end{abstract}

\begin{abstract}{english}
Human motion forecasting plays a crucial role in applications such as sports analysis, injury prevention, and game strategy optimization. However, accurately predicting motion in dynamic, real-time environments like basketball games remains a significant challenge due to the indeterministic and highly variable nature of human movement. Traditional forecasting models, such as Kalman and particle filters, are often ineffective in such contexts because they fail to capture the non-linear and stochastic nature of fast-paced sports. Recent advancements in deep learning models, particularly Recurrent Neural Networks (RNNs) like Long Short-Term Memory (LSTM) and Legendre Memory Units (LMUs), have shown promise in overcoming these limitations. However, these models still struggle with memory retention and processing large input data sequences.

This thesis explores the potential of Transformer-based models, known for their self-attention mechanism, to enhance the accuracy and robustness of human motion forecasting. The self-attention mechanism allows Transformers to capture long-range dependencies and nonlinearities in the data, making them particularly suitable for complex, multivariate, and multi-task motion prediction. We compare the performance of LSTM, LMU, and Transformer architectures on dynamic sports datasets, focusing on NBA player movements. Specifically, we investigate how these models handle varying historical contexts, forecast horizons, and input data configurations, including position-only, velocity-only, and combined inputs. Additionally, the thesis examines the impact of multivariate versus univariate modeling approaches, as well as the generalizability of these models across different teams using transfer learning techniques.

Our results demonstrate that Transformer models outperform traditional recurrent architectures in terms of accuracy, robustness, and forecast horizon when applied to multivariate sports data. However, they also introduce higher computational complexity. We also found that incorporating both positional and velocity data leads to more accurate predictions, and that multivariate models perform better than their univariate counterparts. Through a series of ablation studies, we identify the optimal configurations for human motion forecasting in sports applications. This work offers insights into how AI models, particularly Transformers, can be adapted to real-time, high-stakes environments and lays the groundwork for future developments in the field of sports analytics.
\end{abstract}
