\begin{abstract}{german}
Die Vorhersage menschlicher Bewegungen spielt eine entscheidende Rolle in Bereichen wie der Sportanalyse (SA), der Verletzungsprävention und der Optimierung von Spielstrategien. Die genaue Vorhersage von Bewegungen in dynamischen, Echtzeitumgebungen, wie beispielsweise bei Basketballspielen, stellt jedoch eine erhebliche Herausforderung dar, da menschliche Bewegungen oft unvorhersehbar und stark variabel sind. Traditionelle Vorhersagemodelle, wie Kalmanfilter (KF) und Partikelfilter (PF), erweisen sich in solchen Kontexten häufig als ineffektiv, da sie die nichtlinearen und stochastischen Eigenschaften schneller Sportarten nicht erfassen können. Neuere Fortschritte rekurrenten neuronalen Netzen (RNNs) wie Long Short-Term Memory (LSTM) und Legendre Memory Units (LMUs), haben vielversprechende Ansätze gezeigt, diese Einschränkungen zu überwinden. Dennoch haben auch diese Modelle Probleme mit der Speicherung langer Zeitfolgen und der Verarbeitung gro{\ss}er~Datensätze.

Wir untersuchen in dieser Arbeit das Potenzial von Transformer-basierten Modellen, die durch ihren Self-Attention-Mechanismus (SAM) bekannt sind, um die Genauigkeit und Robustheit der Vorhersage menschlicher Bewegungen zu verbessern. SAM ermöglicht es Transformern, gro{\ss}e zeitliche Abhängigkeiten und Nichtlinearitäten in den Daten zu erfassen, was sie besonders geeignet für komplexe, multivariate und Multi-Task-Bewegungsvorhersagen macht. Wir vergleichen die Leistung von LSTM-, LMU- und Transformer-Architekturen anhand von dynamischen Sportdatensätzen, mit besonderem Fokus auf die Bewegungen von NBA-Spielern. Insbesondere untersuchen wir, wie diese Modelle mit unterschiedlichen historischen Kontexten, Vorhersagehorizonten und Eingabedatenkonfigurationen – einschlie{\ss}lich positionsbasierter, geschwindigkeitsbasierter und kombinierter Eingaben – umgehen. Zusätzlich testen wir die Generalisierbarkeit der Modelle über verschiedene Teams hinweg, indem wir sie beispielsweise auf den Los Angeles Lakers (LAL) trainieren und die Generalisierung auf die Toronto Raptors (TOR)~prüfen. 

Unsere Ergebnisse zeigen, dass Transformer-Modelle die rekurrenten Architekturen hinsichtlich Genauigkeit, Robustheit und Vorhersagehorizont bei der Anwendung auf multivariate Sportdaten übertreffen. Allerdings gehen sie auch mit einer höheren Rechenkomplexität einher. Es zeigte sich zudem, dass die Kombination von Positions- und Geschwindigkeitsdaten zu genaueren Vorhersagen führt und multivariate Modelle gegenüber univariaten Ansätzen überlegen sind. Durch eine Reihe von Ablationsstudien identifizieren wir die optimalen Konfigurationen für die Vorhersage menschlicher Bewegungen in Sportanwendungen. Diese Arbeit liefert Einblicke, wie KI-Modelle, insbesondere Transformer, an Echtzeit-Umgebungen mit hohen Anforderungen angepasst werden können, und bildet die Grundlage für zukünftige Entwicklungen im Bereich der~SA.
\end{abstract}

\begin{abstract}{english}
Predicting human movements plays a crucial role in areas such as sports analysis (SA), injury prevention, and optimization of game strategies. However, accurately predicting movements in dynamic, real-time environments, such as basketball games, presents a significant challenge because human movements are often unpredictable and highly variable. Traditional prediction models, such as Kalman filters (KF) and particle filters (PF), often prove ineffective in these contexts as they fail to capture the nonlinear and stochastic characteristics of fast-paced sports. Recent advances in deep learning models, particularly recurrent neural networks (RNNs) such as Long Short-Term Memory (LSTM) and Legendre Memory Units (LMUs), have shown promising approaches to overcoming these limitations. Nevertheless, these models also face issues with storing long sequences and processing large datasets.

In this work, we explore the potential of Transformer-based models, known for their Self-Attention Mechanism (SAM), to improve the accuracy and robustness of human movement prediction. SAM enables Transformers to capture long-range temporal dependencies and nonlinearities in the data, making them particularly suited for complex, multivariate, and multi-task movement predictions. We compare the performance of LSTM, LMU, and Transformer architectures using dynamic sports datasets, with a particular focus on NBA player movements. Specifically, we investigate how these models handle different historical contexts, prediction horizons, and input data configurations—including position-based, speed-based, and combined inputs. Additionally, we test the generalizability of the models across different teams by training them, for example, on the Los Angeles Lakers (LAL) and evaluating generalization on the Toronto Raptors~(TOR).

Our results show that Transformer models outperform recurrent architectures in terms of accuracy, robustness, and prediction horizon when applied to multivariate sports data. However, they also come with higher computational complexity. We also found that combining position and speed data leads to more accurate predictions and that multivariate models are superior to univariate approaches. Through a series of ablation studies, we identify the optimal configurations for predicting human movements in sports applications. This work provides insights into how AI models, particularly Transformers, can be adapted to real-time environments with high demands and lays the groundwork for future developments in the field of SA.
\end{abstract}


