\chapter{Theoretical Background}

This chapter presents a comprehensive overview of the foundational strategies in predictive modeling and motion forecasting, laying the groundwork for accurate prediction and analysis. It begins with an exploration of linear models in Section \ref{sect:linear}, highlighting their simplicity and inherent limitations in handling complex dynamic environments. Following this, Section \ref{sect:rnn} delves into recurrent models, with a particular focus on \gls{lstm} in Section \ref{sect:lstm} and \gls{lmu} in Section \ref{sect:lmu}, which overcome several of the challenges faced by traditional recurrent methods.

Section \ref{sect:trafo} then explores attention-based Transformer models, examining key components such as positional embeddings, encoder blocks, \gls{mha}, and \gls{sdpa}, along with the role of layer normalization in the Transformer's architecture. The chapter proceeds with Section \ref{sect:bitnet}, which introduces BitNet, an enhancement that significantly improves the performance of Transformer models. Additionally, the chapter addresses why TimeGPT, discussed in Section \ref{sect:timegpt}, is no longer considered a competitor in this domain.

Finally, Section \ref{sec:performance} is dedicated to performance measurement, detailing the metrics and evaluation techniques employed to assess the effectiveness of the various models covered throughout the chapter.

\input{contents/Basics/Network}
