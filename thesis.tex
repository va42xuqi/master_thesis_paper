\documentclass{thesis}

%%%%%%%%%% LANGUAGE SELECTION %%%%%%%%%%

% Use German. Swap the % before the following two lines for an english thesis:
%\Germantrue
\Germanfalse

%%%%%%%%%% PACKAGES %%%%%%%%%%
% Put additional packages here
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\usepackage{booktabs}
\usepackage{float}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{verbatim}
\usepackage{scrhack}
\usepackage{needspace}
\usepackage{ragged2e}
\usepackage{placeins}
\usepackage{stfloats}
\usepackage{subcaption}
\usepackage[ruled,vlined]{algorithm2e}


%%%%%%%%%% CONFIGURATION %%%%%%%%%%
\thesistype{\Master} % Or \Master or a custom string
\title{Transformers are all you need? Forecasting human motion with attention!}
\author{Denis Gosalci}
\authorgender{m} % or "f"
\birthdate{10.05.1999}
\birthloc{Nuremberg}
\thesisstart{01.04.2024}
\thesisend{30.09.2024}
\institute{\FAUIPSG}
\instituteloc{Erlangen}
\thesissupervisor{Tobias Feigl} % Can be used multiple times

% Fill in the following according to the entry in UnivIS for the thesis
\begin{thesisbackground}
Forecasting methods for the stock market, electricity prices, or weather and climate are ubiquitous and are extensively researched [1]. However, forecasting human motion in sports is only sparsely researched. Here too, forecasting technology is crucial to compress system delays at an early stage, reduce injuries, achieve tactical advantages [2] and, in short, improve analysis before, during, and post-game [3]. For instance, coaches draw more effective decisions, such as switching players for health reasons due to irregular motion. The demand for forecasting methods is currently increasing with newly emerging communication platforms, e.g., 6G, and the Internet of Things. However, forecasting human motion, especially their positions, is a difficult task as human motion is practically indeterministic. Scenarios with fast and unpredictable human motion, e.g., in sports such as NBA basketball, render forecasting virtually impossible. So, experts explore forecasting models that can accurately, reliably, promptly, and as far as possible forecast people’s future motion in complex and dynamic environments. For this purpose, classic Bayesian methods such as Kalman and particle filters (KF, PF) [4] were developed. However, these linear models do not remember the motion history necessary to forecast more than just a single time step of the future. In sport, KF and PF are practically ineffective when people move with rapid changes in speed and acceleration and influence each other (tactically) during the course of the game. That is why promising AI approaches have recently been explored. These methods have the potential to exploit nonlinear patterns in dynamic, complex motion that even experts miss as their nonlinear learning capabilities handle the “stochastic” nature of sports motion. The idea is that artificial intelligence (AI) observes, interprets, and learns the movement patterns, their connections and influences, remembers the history and evolution of patterns so that, equipped with this knowledge, the AI is able to forecast complex and dynamic (indeterministic) motion. Recently, Vaswani et al. [5] introduced the most prominent Transformer model that leverages the self-attention mechanism to increase the number of memorable time steps, thus surpassing traditional recurrent neural networks (RNNs) such as LSTMs [6] and LMU [7] w.r.t. accuracy and robustness. Transformers offer the advantages of convolutional networks (optimal feature extraction) and RNNs (memory) and are robust against very large input data spaces with very high information complexity and dimensionality. A well-known example of a Transformer-based model is Chat-GPT for natural language data processing [8] or TimeGPT for time series data processing [1]. However, there is currently no Transformer model for (multivariate and multi-task) forecasting of motion in sport.\\
\end{thesisbackground} 
\begin{thesistask}
The literature leaves it unclear why more memory of LMU and attention cells does not necessarily result in higher accuracy and robustness than LSTM cells. Therefore, the student will examine the performance of LSTM and LMU cells and Transformer (attention) architecture on NBA data and evaluate the impact of time scale, memory, and parameters on accuracy, robustness, uncertainty, and forecast horizon. Since previous work trained individual Transformer models for each pedestrian to estimate person-specific positions, the information-rich (social) context between pedestrians is lost. It is unclear how (social) interactions and contexts can be represented in Transformer models. It is also unclear how and whether Transformers lead to more accurate, robust, and comprehensive forecasts than the state-of-the-art. The student will thus explore various LSTM, LMU, classic Transformer, BitNet, and (optionally) RetNet models for multivariate and multi-task forecasting of motion in sports. The student investigates the effects of input dimensions (a single player up to a maximum of 10 NBA players per game) on the accuracy, robustness, uncertainty, and forecast horizon of Transformer-like models. Since it is also unclear how and whether precompiled Transformers such as TimeGPT can be adapted on multivariate and multi-tasking human movement forecasting in sports applications. The student will investigate whether and how we can define and generate input embeddings with at least two players so that TimeGPT may forecast at least the positions of a single player. The student will also examine the effects of splitting the position into univariate input embeddings, i.e., x- and y-coordinates, to forecast only x- and y-coordinates. He will compare different embeddings and optimizations of LSTM, LMU, a classic Transformer, TimeGPT, BitNet, and (optionally) RetNet and reports performance and computational complexity. The student will also examine how these models can be adapted or fine-tuned to sports motion. The student will conduct an ablation study that optimizes and evaluates all (possible) methods in terms of: velocity versus position input information; length of historical context versus forecast horizon; impact on the forecast horizon; training and inference time. For all benchmarks, the student employs at least the following metrics: Mean Absolute Error (MAE), Root Mean Square Error (RMSE), Average Displacement Error (ADE), and Final Displacement Error (FDE)[14]. A graphical evaluation compares the positioning results.
\end{thesistask}

\thesismilestone{3 PW Literature review.}
\thesismilestone{3 PW Preprocessing of data sets, e.g., NBA [15].}
\thesismilestone{4 PW Implementation of a (parameterizable) software framework for forecasting human motion.}
\thesismilestone{6 PW Adaptation and implementation of methods such as LSTM [6], LMU [7], Transformer [5], TimeGPT [1], BitNet [9], and (optionally) RetNet [10].}
\thesismilestone{4 PW Optimization and evaluation of the methods along an ablation study, e.g., effects of input (contextual) dimension, computational complexity, forecast horizon, storage capacity, and metrics such as Mean Absolute Error (MAE), Root Mean Square Error (RMSE), Average Displacement Error (ADE), Final Displacement Error (FDE) and Average Non-linear displacement error (NL-ADE) [14].}
\thesismilestone{4 PW Transcript.}


\thesisliterature{[1] A. Garza and M. Mergenthaler-Canseco, ``TimeGPT-1'' arXiv: 2310.03589 [cs.LG], pp. 1-–12, 2023. doi: 10.48550/arXiv.2310.03589.}
\thesisliterature{[2] L. Lamas, J. Barrera, G. Otranto, and C. Ugrinowitsch, ``Invasion Team Sports: Strategy and Match Modeling'' Intl. Jo. of Performance Analysis in Sport, vol. 14, no. 1, pp. 307–-329, 2014. doi: 10.1080/24748668.2014.11868723.}
\thesisliterature{[3] J. Gudmundsson and M. Horton, ``Spatio-Temporal Analysis of Team Sports'' ACM Computing Surveys (CSUR), vol. 50, no. 2, pp. 1–-42, 2017. \\ doi: 10.48550/arXiv.1602.06994.}
\thesisliterature{[4] T. Feigl, ``Datengetriebene Methoden zur Bestimmung von Position und Orientierung in funk- und trägheitsbasierter Koppelnavigation'' Dr.-Ing. Dissertation, Technische Fakultät der Friedrich-Alexander-Universität Erlangen-Nürnberg, 2021.}
\thesisliterature{[5] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, ``Attention Is All You Need'' arXiv:1706.03762 [cs.CL], pp. 1–-15, 2017. doi: 10.48550/arXiv.1706.03762.}
\thesisliterature{[6] S. Hochreiter and J. Schmidhuber, ``Long short-term memory'' Neural computation, vol. 9, no. 8, pp. 1735–-1780, 1997. doi: 10.1162/neco.1997.9.8.1735.}
\thesisliterature{[7] A. Voelker, I. Kaji\'{c} and C. Eliasmith, ``Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks'' in Proc. of Conf. Advances in Neural Information Processing Systems (NIPS), Vancouver, Canada, 2019, pp. 15544-–15553. [Online]. Available: https://papers.nips.cc/paper\_files/paper/2019/file/\\952285b9b7e7a1be5aa7849f32ffff05-Paper.pdf.}
\thesisliterature{[8] OpenAI, ``GPT-4 Technical Report'' arXiv:2303.08774v3 [cs.CL], pp. 1–-100, 2023. doi: 10.48550/arXiv.2303.08774.}
\thesisliterature{[9] H. Wang, S. Ma, L. Dong, S. Huang, H. Wang, L. Ma, F. Yang, R. Wang, Y. Wu, and F. Wei, ``BitNet: Scaling 1-bit Transformers for Large Language Models'' arXiv: 2310.11453 [cs.CL], pp. 1–-14, 2023. doi: 10.48550/arXiv.2310.11453.}
\thesisliterature{[10] Y. Sun, L. Dong, S. Huang, S. Ma, Y. Xia, J. Xue, J. Wang, and F. Wei, ``Retentive Network: A Successor to Transformer for Large Language Models'' arXiv: 2307.08621 [cs.CL], pp. 1–-14, 2023. doi: 10.48550/arXiv.2307.08621.}
\thesisliterature{[11] S. Hauri, N. Djuric, V. Radosavljevic, and S. Vucetic, ``Multi-Modal Trajectory Prediction of NBA Players'' in Proc. of IEEE Winter Conf. on Applications of Computer Vision (WACV), Waikoloa, HI, 2021, pp. 1639–-1648. \\ doi: 10.1109/WACV48630.2021.00168.}
\thesisliterature{[12] D. Gosalci, ``Legendre Memory Unit'' Bachelor’s Thesis, Technische Hochschule Georg-Simon-Ohm Nürnberg, 2021.}
\thesisliterature{[13] F. Giuliari, I. Hasan, M. Cristani, and F. Galasso, ``Transformer Networks for Trajectory Forecasting'' arXiv:2003.08111 [cs.CV], pp. 1–-14, 2020. doi: 10.48550/arXiv.2003.08111.}
\thesisliterature{[14] K. Xu, Z. Qin, G. Wang, K. Huang, S. Ye, and H. Zhang, ``Collision-Free LSTM for Human Trajectory Prediction'' in Proc. of MultiMedia Modeling, Bangkok, Thailand, 2018, pp. 106–-116. doi: 10.1007/978-3-319-73603-7\_9.}
\thesisliterature{[15] K. Linou, D. Linou, and M. de Boer, ``NBA Player Movements Dataset'' 2016. [Online]. Available: https://github.com/linouk23/NBA-Player-Movements.}



%%%%%%%%%% BIBLIOGRAPHY %%%%%%%%%%

% Put all references in the file references.bib in BibTeX format. Please pay
% attention to some common errors when writing references:
% https://www.ece.ucdavis.edu/~jowens/biberrors.html
\addbibresource{references.bib}



%%%%%%%%%% DOCUMENT %%%%%%%%%%
\begin{document}


%%%%%%%%%% COVER %%%%%%%%%%
% Cover muss separat kompiliert werden über "cover.tex" mit xetex / luatex compiler
% Das pdf muss herunter- und hier wieder hochgeladen werden!
\includepdf[pages=-]{cover.pdf}

\MakeTitlePage

\MakePledgePage % Or (only in rare cases!) \MakePledgePage[nogrant]

\MakeTaskPage

\include{contents/0_abstract}

\listoftodos

\tableofcontents
\listoffigures
\listofalgorithms
\listoftables

\include{contents/abbreviations}
\include{contents/symbols}

\setlength{\parindent}{0pt} % damit die Zeilen immer bei ganz links anfangen

\mainmatter
\include{contents/1_introduction}
\include{contents/2_related_work}
\include{contents/3_basics}
\include{contents/4_methods}
\include{contents/5_experimental_setup}
\include{contents/6_results}
\include{contents/7_discussion}
\include{contents/8_conclusion}
\include{contents/9_outlook_fw}

\printglossaries
\printbibliography
\end{document}
